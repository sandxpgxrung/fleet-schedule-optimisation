{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location dataframe\n",
    "location_df = pd.read_csv('SaO_Optilandia_resub_locations.csv')\n",
    "\n",
    "# links dataframe\n",
    "links_df = pd.read_csv('SaO_Optilandia_resub_links.csv')\n",
    "\n",
    "# extract lorry data from json\n",
    "lorry_data = json.load(open('SaO_Optilandia_resub_depot_lorries.json', 'r'))\n",
    "\n",
    "# set count to 0\n",
    "k = 0\n",
    "\n",
    "# initialise lorry list\n",
    "lorry = []\n",
    "\n",
    "# loop -> set i to the respective lorry key\n",
    "for i in lorry_data.keys():\n",
    "    # set j to the the number of lorries at key 'i'\n",
    "    for j in range(0, len(lorry_data[i])):\n",
    "        # append each lorry in lorry_data to lorry list\n",
    "        lorry.append(pd.DataFrame(lorry_data[i][j], index=[k]))\n",
    "        # accumulate index\n",
    "        k += 1\n",
    "\n",
    "# lorry dataframe\n",
    "lorry_df = pd.concat(lorry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting required column \n",
    "location_df['required'] = location_df['capacity']-location_df['level']\n",
    "\n",
    "# displaying rows where is_customer true\n",
    "location_df[location_df['is_customer']==True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'depot' column to lorry_df and updae with the lorry's relative depot \n",
    "lorry_df['depot'] = lorry_df.lorry_id.apply(lambda x: x.split('-')[0])\n",
    "lorry_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of depot locations (where nodes == depot)\n",
    "depot_locations = np.where(location_df.is_depot)[0]\n",
    "\n",
    "# list of customer locations (where nodes == customers)\n",
    "customer_locations = np.where(location_df.is_customer)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports + visualising the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# parwise distance calculation for each node\n",
    "euclidean = squareform(pdist(location_df[['x', 'y']]))\n",
    "\n",
    "# edges list initialisation\n",
    "edges = []\n",
    "\n",
    "# loop through links_df rows\n",
    "for _, (i, j) in links_df.iterrows():\n",
    "    # append node at i, node at j, and their pairwise distance to edges\n",
    "    edges.append((i, j, euclidean[i, j]))\n",
    "\n",
    "# pos dict intialisation\n",
    "pos = {}\n",
    "\n",
    "# loop through location_df rows\n",
    "for k, v in location_df[['x', 'y']].iterrows():\n",
    "    # update pos dict with array of k, v \n",
    "    pos.update({k:v.values})\n",
    "\n",
    "# initialise depot_labels dict\n",
    "depot_labels = {}\n",
    "\n",
    "# loop throgugh depot_locations\n",
    "for i in depot_locations:\n",
    "    # update depot_labels dict with {i:i}\n",
    "    depot_labels.update({i:i})\n",
    "\n",
    "# initialise customer_labels dict\n",
    "customer_labels = {}\n",
    "\n",
    "# loop through customer_locations\n",
    "for i in customer_locations:\n",
    "    # update customer_labels dict with {i:i}\n",
    "    customer_labels.update({i:i})\n",
    "\n",
    "# initialise nx Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# feed node list to G\n",
    "G.add_nodes_from(location_df['id'].to_numpy())\n",
    "\n",
    "# feed edges list to G\n",
    "G.add_weighted_edges_from(edges)\n",
    "\n",
    "# resize figure \n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# sketch graph\n",
    "nx.draw(G, pos=pos, node_size=40)\n",
    "\n",
    "# label depot nodes\n",
    "nx.draw_networkx_labels(G, pos, depot_labels)\n",
    "\n",
    "# label customer nodes\n",
    "nx.draw_networkx_labels(G, pos, customer_labels)\n",
    "\n",
    "# mark depot nodes\n",
    "nx.draw_networkx_nodes(G, pos=pos, nodelist=depot_locations, node_color='r', node_size=400, alpha=0.9)\n",
    "\n",
    "# mark customer nodes\n",
    "nx.draw_networkx_nodes(G, pos=pos, nodelist=customer_locations, node_color='g', node_size=200, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clustering nodes ... (nearest-neighbour approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialise cluster dict\n",
    "cluster = {124:[], 127:[], 167:[], 523:[]}\n",
    "\n",
    "# intialise nodes list\n",
    "nodes = [] \n",
    "\n",
    "# loop through each node in customer_locations\n",
    "for node in customer_locations:\n",
    "    # check if node in nodes\n",
    "    if node not in nodes:\n",
    "        # initialise dist list\n",
    "        dist = []\n",
    "        # loop through each depot key\n",
    "        for depot in cluster.keys():\n",
    "            # append euclidean weights to dist \n",
    "            dist.append(euclidean[node, depot])\n",
    "        # get shortest distance\n",
    "        shortestDist = min(dist)\n",
    "        # match shortest distance to equivalent node index\n",
    "        nearestDepotIndex = np.where(euclidean[node]==shortestDist)\n",
    "        # add node to relative nearest depot location\n",
    "        cluster[int(nearestDepotIndex[0])].append(node)\n",
    "        # track applied nodes\n",
    "        nodes.append(node)\n",
    "        # clear dist\n",
    "        dist.clear()\n",
    "\n",
    "# print allocated nodes to relative cluster points (depot locations)\n",
    "print(cluster)\n",
    "\n",
    "# clear nodes list\n",
    "nodes.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(breadth-first-search used for pathfinding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing from A to B using recursive Breadth-First-Search based algorithm (pathfinding)\n",
    "def routing(currState, toState, edges):\n",
    "    # intialise visitedState list for tracking node traversal\n",
    "    visitedState = [currState]\n",
    "    # initialise visitedEdge list for tracking edge traversal\n",
    "    visitedEdge = []\n",
    "    # intialise edgeMemory list for storing the explored edges\n",
    "    edgeMemory = []\n",
    "    # intialise queue list for choosing the central node for next traversal\n",
    "    queue = [currState]\n",
    "\n",
    "    # loop while toState is not found\n",
    "    while currState != toState:\n",
    "        # remove and store the last element of the queue list as q\n",
    "        q = queue.pop(0)\n",
    "        \n",
    "        # intialise currEdges dict which holds the next set of edges for traversals\n",
    "        currEdges = {}\n",
    "        # get the nodes at each edge, where either nodes are equivalent to q\n",
    "        for edge in list(np.where(links_df[['id1', 'id2']]==q)[0]):\n",
    "            # update the dict with the relative edge key and the node pairs\n",
    "            currEdges.update({edge:[edges[edge][0], edges[edge][1]]})\n",
    "        \n",
    "        # store the explored edges in edgeMemory list\n",
    "        edgeMemory.append(currEdges)\n",
    "\n",
    "        # loop through each edge in currEdges\n",
    "        for edge in currEdges:\n",
    "            # check if the edge has been visited \n",
    "            if edge not in visitedEdge:\n",
    "                # if not visited then add the edge to visitedEdge\n",
    "                visitedEdge.append(edge)\n",
    "                # check the node index in the edge that has not been visited \n",
    "                if currEdges[edge][0] not in visitedState and currEdges[edge][1] in visitedState:\n",
    "                    # set currState to the unvisited node \n",
    "                    currState = currEdges[edge][0]\n",
    "                    # mark the node in currState as visited \n",
    "                    visitedState.append(currState)\n",
    "                    # add new currState to queue \n",
    "                    queue.append(currState)\n",
    "                    # check if toState reached\n",
    "                    if currState == toState:\n",
    "                        # set currState to toState\n",
    "                        currState = toState\n",
    "                        # end loop\n",
    "                        break \n",
    "                # similar to above but in the context of different index position of the node that has not been visited\n",
    "                if currEdges[edge][1] not in visitedState and edges[edge][0] in visitedState:\n",
    "                    currState = currEdges[edge][1]\n",
    "                    visitedState.append(currState)\n",
    "                    queue.append(currState)\n",
    "                    if currState == toState:\n",
    "                        currState = toState\n",
    "                        break\n",
    "    \n",
    "    # set startState as the first node in visitedState list\n",
    "    startState = visitedState[0]\n",
    "    # set lastQ as the toState for tracking q node from end of order\n",
    "    lastQ = [toState]\n",
    "    # intialise backtrace list for backtracing the edges from edgeMemory\n",
    "    backtrace = []\n",
    "    # initialise nodetrace list for backtracing the nodes from edgeMemory\n",
    "    nodetrace = []\n",
    "\n",
    "    # intialise edgeMemoryReversed for reordering edgeMemory \n",
    "    edgeMemoryReversed = []\n",
    "    # loop through each index between range 0 and length of edgeMemory\n",
    "    for i in range (0, len(edgeMemory)):\n",
    "        # set endElement to the last element in edgeMemory\n",
    "        endElement = edgeMemory.pop(-1)\n",
    "        # add the endElement to edgeMemoryReversed\n",
    "        edgeMemoryReversed.append(endElement)\n",
    "    \n",
    "    # while last element in lastQ is not equivalent to the startState\n",
    "    while lastQ[-1] != startState:\n",
    "        # loop through each edge options in edgeMemoryReversed\n",
    "        for edgeOpt in edgeMemoryReversed:\n",
    "            # loop through each edge from as keys of the edge options\n",
    "            for edge in edgeOpt.keys():\n",
    "\n",
    "                # check if last element of lastQ is in the set of edge options given the edge\n",
    "                if lastQ[-1] in edgeOpt[edge]:\n",
    "                    # add the edge to backtrace \n",
    "                    backtrace.append(edge)\n",
    "                    # check index of node which matches the lastQ element \n",
    "                    if lastQ[-1] == edgeOpt[edge][0] and lastQ[-1] != edgeOpt[edge][1]:\n",
    "                        # update lastQ as the the node which does not match the lastQ element\n",
    "                        lastQ.append(edgeOpt[edge][1])\n",
    "                        # add the node to nodetrace\n",
    "                        nodetrace.append(edgeOpt[edge][1])\n",
    "                        # return to while iterate\n",
    "                        break\n",
    "                        # similar to above but in the context of different index postion of the matching node with lastQ element\n",
    "                    if lastQ[-1] != edgeOpt[edge][0] and lastQ[-1] == edgeOpt[edge][1]:\n",
    "                        lastQ.append(edgeOpt[edge][0])\n",
    "                        nodetrace.append(edgeOpt[edge][0])\n",
    "                        break\n",
    "    \n",
    "    # re-ordering edges from start to end\n",
    "    edgeTraversed = []\n",
    "    for i in range(0, len(backtrace)):\n",
    "        endElement = backtrace.pop(-1)\n",
    "        edgeTraversed.append(endElement)\n",
    "\n",
    "    # re-ordering nodes from start to end \n",
    "    nodeOrder = []\n",
    "    for i in range(0, len(nodetrace)):\n",
    "        endElement = nodetrace.pop(-1)\n",
    "        nodeOrder.append(endElement)\n",
    "    \n",
    "    # adding route weight (distance between nodes) to each traversal made\n",
    "    routeWeight = []\n",
    "    for edge in edgeTraversed:\n",
    "        routeWeight.append(edges[edge][2])\n",
    "\n",
    "    # return the the order in which nodes were visited and the order in which edges were traversed\n",
    "    return nodeOrder, edgeTraversed, routeWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: gives the node order in routing\n",
    "print(f'node order: {routing(124, 10, edges)[0]}')\n",
    "# test: gives the traversed edges in routing\n",
    "print(f'edge order: {routing(124, 10, edges)[1]}')\n",
    "# test: gives the weight routes for traversed\n",
    "print(f'weights: {routing(124, 10, edges)[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genetic algorithm for exploring solution set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initailise routes dict \n",
    "routes = {}\n",
    "\n",
    "# loop through lorry_df index\n",
    "for i in lorry_df.index:\n",
    "    # for each lorry, initialise journey by appending start (depot) and capacity (self)\n",
    "    routes.update({lorry_df['lorry_id'][i]:[(lorry_df.depot[i]), lorry_df.capacity[i]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display routes so far\n",
    "routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import random\n",
    "\n",
    "# function to generate a random path order (consider as genome) for each depot # returns a dict \n",
    "def randomPathArrangement(cluster):\n",
    "    randomPathArr = {}\n",
    "    customerAllocation = cluster\n",
    "    for depot in depot_locations:\n",
    "        randomPathArr.update({depot:random.sample(customerAllocation[depot], len(customerAllocation[depot]))})\n",
    "    return randomPathArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(randomPathArrangement(cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate multiple initial solutions (population) # returns genomes (individual solution) in a list\n",
    "def population_init(cluster, size):\n",
    "    population = []\n",
    "    for i in range(0, size):\n",
    "        population.append(randomPathArrangement(cluster))\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test (cluster = {depot:customers} (genome), size = NUMBER OF GENOMES)\n",
    "tempPop = population_init(cluster, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "for i in tempPop:\n",
    "    print(f'\\ntempPop:\\n{i}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return populationFitness at each index...\n",
    "def fitness(population, edges):\n",
    "\n",
    "    # to store each weight per traversal made between two nodes (when calculating traversal distance per depot)\n",
    "    genomeWeight = []\n",
    "\n",
    "    # for a possible solution in the set of solutions (genome in population)\n",
    "    for genome in population:\n",
    "        # initialise genomeFitness dict to store distances at each depot per genome \n",
    "        genomeFitness = {}\n",
    "        for depot in genome.keys():\n",
    "            genomeFitness.update({depot:[]})\n",
    "            nodes = genome[depot]\n",
    "            # loop for length of nodes\n",
    "            for idx in range(len(nodes)-1):\n",
    "                # set nodeA to current loop index\n",
    "                nodeA = nodes[idx]\n",
    "                # set nodeB to next loop index\n",
    "                nodeB = nodes[idx+1]\n",
    "                # get sum of each traversal distance occurence between two nodes | routing() enables this \n",
    "                distance = sum(routing(nodeA, nodeB, edges)[2])\n",
    "                genomeFitness[depot].append(distance)\n",
    "        # update popFitness list with each genome and relative fitness values\n",
    "        genomeWeight.append(genomeFitness)\n",
    "\n",
    "    genomeWeights = []\n",
    "    for genome in genomeWeight:\n",
    "        # initialise popWeight list to store total weight at each depot for genome\n",
    "        depotWeight = []\n",
    "        # loop through each depot (chromosome per genome)\n",
    "        for depot in genome:\n",
    "            depotWeight.append({depot:sum(genome[depot])})\n",
    "        genomeWeights.append(depotWeight)\n",
    "    \n",
    "    # to store the weight per index (weight per genome) [each solution indicated by index i.e., 0, 1, 2 etc.]\n",
    "    weightIndex = {}\n",
    "    for idx, genome in enumerate(genomeWeights):\n",
    "        tempIndexStore = []\n",
    "        for depot in genome:\n",
    "            for i in depot.values():\n",
    "                tempIndexStore.append(i)\n",
    "        weightIndex.update({idx:sum(tempIndexStore)})\n",
    "    \n",
    "    return population, weightIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "tempPopWeights = fitness(tempPop, edges)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection() uses weighted selection probability for returning 2 cluster arrangement (2 genomes)\n",
    "def selection(population, popWeights):\n",
    "    # selectionPair defines a list containing # of genomes selected through concept of roulette-wheel (in this case k=2). \n",
    "    selectionPair = random.choices(population=population, weights=popWeights, k=2)\n",
    "    # return the selectionPair (in form list)\n",
    "    return selectionPair "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "tempSelectionPair = selection(tempPop, tempPopWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7370fec6be111b608cc34d50d3192bee79c029552a2dcb4c2363d9b827a0176"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
