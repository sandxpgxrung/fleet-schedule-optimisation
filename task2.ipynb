{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location dataframe\n",
    "location_df = pd.read_csv('SaO_Optilandia_resub_locations.csv')\n",
    "\n",
    "# links dataframe\n",
    "links_df = pd.read_csv('SaO_Optilandia_resub_links.csv')\n",
    "\n",
    "# extract lorry data from json\n",
    "lorry_data = json.load(open('SaO_Optilandia_resub_depot_lorries.json', 'r'))\n",
    "\n",
    "# set count to 0\n",
    "k = 0\n",
    "\n",
    "# initialise lorry list\n",
    "lorry = []\n",
    "\n",
    "# loop -> set i to the respective lorry key\n",
    "for i in lorry_data.keys():\n",
    "    # set j to the the number of lorries at key 'i'\n",
    "    for j in range(0, len(lorry_data[i])):\n",
    "        # append each lorry in lorry_data to lorry list\n",
    "        lorry.append(pd.DataFrame(lorry_data[i][j], index=[k]))\n",
    "        # accumulate index\n",
    "        k += 1\n",
    "\n",
    "# lorry dataframe\n",
    "lorry_df = pd.concat(lorry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting required column \n",
    "location_df['required'] = location_df['capacity']-location_df['level']\n",
    "\n",
    "# displaying rows where is_customer true\n",
    "location_df[location_df['is_customer']==True].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'depot' column to lorry_df and updae with the lorry's relative depot \n",
    "lorry_df['depot'] = lorry_df.lorry_id.apply(lambda x: x.split('-')[0])\n",
    "lorry_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of depot locations (where nodes == depot)\n",
    "depot_locations = np.where(location_df.is_depot)[0]\n",
    "\n",
    "# list of customer locations (where nodes == customers)\n",
    "customer_locations = np.where(location_df.is_customer)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports + visualising the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# parwise distance calculation for each node\n",
    "euclidean = squareform(pdist(location_df[['x', 'y']]))\n",
    "\n",
    "# edges list initialisation\n",
    "edges = []\n",
    "\n",
    "# loop through links_df rows\n",
    "for _, (i, j) in links_df.iterrows():\n",
    "    # append node at i, node at j, and their pairwise distance to edges\n",
    "    edges.append((i, j, euclidean[i, j]))\n",
    "\n",
    "# pos dict intialisation\n",
    "pos = {}\n",
    "\n",
    "# loop through location_df rows\n",
    "for k, v in location_df[['x', 'y']].iterrows():\n",
    "    # update pos dict with array of k, v \n",
    "    pos.update({k:v.values})\n",
    "\n",
    "# initialise depot_labels dict\n",
    "depot_labels = {}\n",
    "\n",
    "# loop throgugh depot_locations\n",
    "for i in depot_locations:\n",
    "    # update depot_labels dict with {i:i}\n",
    "    depot_labels.update({i:i})\n",
    "\n",
    "# initialise customer_labels dict\n",
    "customer_labels = {}\n",
    "\n",
    "# loop through customer_locations\n",
    "for i in customer_locations:\n",
    "    # update customer_labels dict with {i:i}\n",
    "    customer_labels.update({i:i})\n",
    "\n",
    "# initialise nx Graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# feed node list to G\n",
    "G.add_nodes_from(location_df['id'].to_numpy())\n",
    "\n",
    "# feed edges list to G\n",
    "G.add_weighted_edges_from(edges)\n",
    "\n",
    "# resize figure \n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# sketch graph\n",
    "nx.draw(G, pos=pos, node_size=40)\n",
    "\n",
    "# label depot nodes\n",
    "nx.draw_networkx_labels(G, pos, depot_labels)\n",
    "\n",
    "# label customer nodes\n",
    "nx.draw_networkx_labels(G, pos, customer_labels)\n",
    "\n",
    "# mark depot nodes\n",
    "nx.draw_networkx_nodes(G, pos=pos, nodelist=depot_locations, node_color='r', node_size=400, alpha=0.9)\n",
    "\n",
    "# mark customer nodes\n",
    "nx.draw_networkx_nodes(G, pos=pos, nodelist=customer_locations, node_color='g', node_size=200, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clustering nodes ... (nearest-neighbour approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialise cluster dict\n",
    "cluster = {124:[], 127:[], 167:[], 523:[]}\n",
    "\n",
    "# intialise nodes list\n",
    "nodes = [] \n",
    "\n",
    "# loop through each node in customer_locations\n",
    "for node in customer_locations:\n",
    "    # check if node in nodes\n",
    "    if node not in nodes:\n",
    "        # initialise dist list\n",
    "        dist = []\n",
    "        # loop through each depot key\n",
    "        for depot in cluster.keys():\n",
    "            # append euclidean weights to dist \n",
    "            dist.append(euclidean[node, depot])\n",
    "        # get shortest distance\n",
    "        shortestDist = min(dist)\n",
    "        # match shortest distance to equivalent node index\n",
    "        nearestDepotIndex = np.where(euclidean[node]==shortestDist)\n",
    "        # add node to relative nearest depot location\n",
    "        cluster[int(nearestDepotIndex[0])].append(node)\n",
    "        # track applied nodes\n",
    "        nodes.append(node)\n",
    "        # clear dist\n",
    "        dist.clear()\n",
    "\n",
    "# print allocated nodes to relative cluster points (depot locations)\n",
    "print(cluster)\n",
    "\n",
    "# clear nodes list\n",
    "nodes.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(breadth-first-search used for pathfinding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing from A to B using recursive Breadth-First-Search based algorithm (pathfinding)\n",
    "def routing(currState, toState, edges):\n",
    "    # intialise visitedState list for tracking node traversal\n",
    "    visitedState = [currState]\n",
    "    # initialise visitedEdge list for tracking edge traversal\n",
    "    visitedEdge = []\n",
    "    # intialise edgeMemory list for storing the explored edges\n",
    "    edgeMemory = []\n",
    "    # intialise queue list for choosing the central node for next traversal\n",
    "    queue = [currState]\n",
    "\n",
    "    # loop while toState is not found\n",
    "    while currState != toState:\n",
    "        # remove and store the last element of the queue list as q\n",
    "        q = queue.pop(0)\n",
    "        \n",
    "        # intialise currEdges dict which holds the next set of edges for traversals\n",
    "        currEdges = {}\n",
    "        # get the nodes at each edge, where either nodes are equivalent to q\n",
    "        for edge in list(np.where(links_df[['id1', 'id2']]==q)[0]):\n",
    "            # update the dict with the relative edge key and the node pairs\n",
    "            currEdges.update({edge:[edges[edge][0], edges[edge][1]]})\n",
    "        \n",
    "        # store the explored edges in edgeMemory list\n",
    "        edgeMemory.append(currEdges)\n",
    "\n",
    "        # loop through each edge in currEdges\n",
    "        for edge in currEdges:\n",
    "            # check if the edge has been visited \n",
    "            if edge not in visitedEdge:\n",
    "                # if not visited then add the edge to visitedEdge\n",
    "                visitedEdge.append(edge)\n",
    "                # check the node index in the edge that has not been visited \n",
    "                if currEdges[edge][0] not in visitedState and currEdges[edge][1] in visitedState:\n",
    "                    # set currState to the unvisited node \n",
    "                    currState = currEdges[edge][0]\n",
    "                    # mark the node in currState as visited \n",
    "                    visitedState.append(currState)\n",
    "                    # add new currState to queue \n",
    "                    queue.append(currState)\n",
    "                    # check if toState reached\n",
    "                    if currState == toState:\n",
    "                        # set currState to toState\n",
    "                        currState = toState\n",
    "                        # end loop\n",
    "                        break \n",
    "                # similar to above but in the context of different index position of the node that has not been visited\n",
    "                if currEdges[edge][1] not in visitedState and edges[edge][0] in visitedState:\n",
    "                    currState = currEdges[edge][1]\n",
    "                    visitedState.append(currState)\n",
    "                    queue.append(currState)\n",
    "                    if currState == toState:\n",
    "                        currState = toState\n",
    "                        break\n",
    "    \n",
    "    # set startState as the first node in visitedState list\n",
    "    startState = visitedState[0]\n",
    "    # set lastQ as the toState for tracking q node from end of order\n",
    "    lastQ = [toState]\n",
    "    # intialise backtrace list for backtracing the edges from edgeMemory\n",
    "    backtrace = []\n",
    "    # initialise nodetrace list for backtracing the nodes from edgeMemory\n",
    "    nodetrace = []\n",
    "\n",
    "    # intialise edgeMemoryReversed for reordering edgeMemory \n",
    "    edgeMemoryReversed = []\n",
    "    # loop through each index between range 0 and length of edgeMemory\n",
    "    for i in range (0, len(edgeMemory)):\n",
    "        # set endElement to the last element in edgeMemory\n",
    "        endElement = edgeMemory.pop(-1)\n",
    "        # add the endElement to edgeMemoryReversed\n",
    "        edgeMemoryReversed.append(endElement)\n",
    "    \n",
    "    # while last element in lastQ is not equivalent to the startState\n",
    "    while lastQ[-1] != startState:\n",
    "        # loop through each edge options in edgeMemoryReversed\n",
    "        for edgeOpt in edgeMemoryReversed:\n",
    "            # loop through each edge from as keys of the edge options\n",
    "            for edge in edgeOpt.keys():\n",
    "\n",
    "                # check if last element of lastQ is in the set of edge options given the edge\n",
    "                if lastQ[-1] in edgeOpt[edge]:\n",
    "                    # add the edge to backtrace \n",
    "                    backtrace.append(edge)\n",
    "                    # check index of node which matches the lastQ element \n",
    "                    if lastQ[-1] == edgeOpt[edge][0] and lastQ[-1] != edgeOpt[edge][1]:\n",
    "                        # update lastQ as the the node which does not match the lastQ element\n",
    "                        lastQ.append(edgeOpt[edge][1])\n",
    "                        # add the node to nodetrace\n",
    "                        nodetrace.append(edgeOpt[edge][1])\n",
    "                        # return to while iterate\n",
    "                        break\n",
    "                        # similar to above but in the context of different index postion of the matching node with lastQ element\n",
    "                    if lastQ[-1] != edgeOpt[edge][0] and lastQ[-1] == edgeOpt[edge][1]:\n",
    "                        lastQ.append(edgeOpt[edge][0])\n",
    "                        nodetrace.append(edgeOpt[edge][0])\n",
    "                        break\n",
    "    \n",
    "    # re-ordering edges from start to end\n",
    "    edgeTraversed = []\n",
    "    for i in range(0, len(backtrace)):\n",
    "        endElement = backtrace.pop(-1)\n",
    "        edgeTraversed.append(endElement)\n",
    "\n",
    "    # re-ordering nodes from start to end \n",
    "    nodeOrder = []\n",
    "    for i in range(0, len(nodetrace)):\n",
    "        endElement = nodetrace.pop(-1)\n",
    "        nodeOrder.append(endElement)\n",
    "    \n",
    "    # adding route weight (distance between nodes) to each traversal made\n",
    "    routeWeight = []\n",
    "    for edge in edgeTraversed:\n",
    "        routeWeight.append(edges[edge][2])\n",
    "\n",
    "    # return the the order in which nodes were visited and the order in which edges were traversed\n",
    "    return nodeOrder, edgeTraversed, routeWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: gives the node order in routing\n",
    "print(f'node order: {routing(124, 10, edges)[0]}')\n",
    "# test: gives the traversed edges in routing\n",
    "print(f'edge order: {routing(124, 10, edges)[1]}')\n",
    "# test: gives the weight routes for traversed\n",
    "print(f'weights: {routing(124, 10, edges)[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "genetic algorithm for exploring solution set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initailise routes dict \n",
    "routes = {}\n",
    "\n",
    "# loop through lorry_df index\n",
    "for i in lorry_df.index:\n",
    "    # for each lorry, initialise journey by appending start (depot) and capacity (self)\n",
    "    routes.update({lorry_df['lorry_id'][i]:[(lorry_df.depot[i]), lorry_df.capacity[i]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display routes so far\n",
    "routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import random\n",
    "\n",
    "# function to generate a random path order (consider as genome) for each depot # returns a dict \n",
    "def randomPathArrangement(cluster):\n",
    "    randomPathArr = {}\n",
    "    customerAllocation = cluster\n",
    "    for depot in depot_locations:\n",
    "        randomPathArr.update({depot:random.sample(customerAllocation[depot], len(customerAllocation[depot]))})\n",
    "    return randomPathArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(randomPathArrangement(cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate multiple initial solutions (population) # returns genomes (individual solution) in a list\n",
    "def population_init(cluster, size):\n",
    "    population = []\n",
    "    for i in range(0, size):\n",
    "        population.append(randomPathArrangement(cluster))\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "tempPop = population_init(cluster, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "for i in tempPop:\n",
    "    print(f'\\ntempPop:\\n{i}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return populationFitness at each index in ascending order (most fit to least fit)\n",
    "def fitness(population, edges):\n",
    "\n",
    "    genomeWeight = []\n",
    "    popFitness = []\n",
    "\n",
    "    # for a possible solution in the set of solutions (genome in population)\n",
    "    for genome in population:\n",
    "        # initialise genomeFitness dict to store distances at each depot per genome \n",
    "        genomeFitness = {}\n",
    "        for depot in genome.keys():\n",
    "            genomeFitness.update({depot:[]})\n",
    "            nodes = genome[depot]\n",
    "            for idx in range(len(nodes)-1):\n",
    "                nodeA = nodes[idx]\n",
    "                nodeB = nodes[idx+1]\n",
    "                # get sum of each traversal distance occurence between two nodes | routing() enables this \n",
    "                distance = sum(routing(nodeA, nodeB, edges)[2])\n",
    "                genomeFitness[depot].append(distance)\n",
    "        # update popFitness list with each genome and relative fitness values\n",
    "        genomeWeight.append(genomeFitness)\n",
    "\n",
    "    for i in genomeWeight.keys():\n",
    "        weight = genomeWeight[i]\n",
    "        \n",
    "\n",
    "            # maybe while loop here? i.e., while nodes fitness not finished eval... pathfind / get weight etc...\n",
    "\n",
    "fitness(tempPop, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genomeWeight = [{124: [764.207424313277, 29.495691125645767, 301.94789440648617, 86.43188547460109, 103.29269324774098, 232.9515137867115, 128.22947032920757, 393.38798000038685, 709.8293768076302, 642.1342308631976, 200.88800501175132, 658.398565959393, 608.9983256352541, 300.02473294735586, 423.23102621587157, 297.6257062845869, 322.8917823783063, 288.27462421455766, 295.6832252823565, 450.6829775380554, 195.65720050348318, 678.7852237779401, 515.7349553313808, 335.91749025969284, 106.43963252156183, 307.76140737295805, 402.066079913012, 643.7135396049698, 658.8047018142415, 768.2248081159325, 813.708674571676, 103.92345376028538, 270.94705340955727, 589.2408968213057, 919.1345237215405, 654.2794983026115, 305.56720328099027, 257.9474081038173, 363.5496130322805, 552.4780590706876, 373.37014964551736], 127: [348.4142861903571, 357.05571442232014, 455.1483505812167, 704.0157008868272, 427.0068674109813, 123.42833303093978, 411.7488382645468, 431.657181083932, 574.4160028703847, 58.06471349776065, 310.7541074832516, 436.98530596353703, 627.5465252964724, 315.9747664603495, 290.38448478394196, 83.2969859130678, 301.470886946703, 457.1135264180963, 126.18430600481119, 314.8792897289423, 497.5738890194847], 167: [1250.272738188178, 140.32987717185813, 196.62339978472454, 279.53262240814354, 543.5169570778512, 444.5473336872827, 341.07549555855906, 731.1340300246841, 627.9768728760403, 413.3193194547188, 513.2440711414912, 765.334939304412, 103.26122839541503, 154.71975635332103, 866.2232691802731, 745.5637553522878, 436.52936832788794, 16.504197856127956, 409.1000414868827, 506.5964086798391, 560.3666894200053, 551.7322751837983, 455.2337440489541, 528.4273490154951, 390.7217940343114, 296.0146898312134, 352.2068989779755, 216.21920758157987, 380.7234794752683, 153.8802697173813, 423.6339377707202, 523.5703057251819, 900.3987567184421, 486.5244553942727, 496.7391510005817, 379.1737183516223, 856.9137593535364, 1039.3115132434327, 1172.69760327064], 523: [397.2511759610576, 76.22882539841802, 466.2029778486828, 436.8572098956855, 79.83964888478602, 49.42764494439111, 274.93391794914004, 440.3321915508013, 412.6407636008004, 515.8336379243157, 486.5016359164346, 437.12467956689915, 482.47541209418864, 643.3140561058268, 445.47109668411093, 719.9332877165486]}, {124: [750.643259987773, 423.37743893418207, 555.4173822966279, 156.98076369543847, 760.771364715029, 725.5565364882851, 857.3776595203979, 486.027363715945, 67.1646045395968, 251.40260165645648, 741.3529645615679, 894.0121350847274, 310.0942213005814, 323.99389891712786, 445.7457690739033, 312.6148483903993, 735.9124686435192, 784.8974894199679, 572.8960429850806, 303.68319466476635, 615.828068389548, 385.11928016382257, 404.06658076196885, 511.95764285989117, 328.2792896865652, 488.21507017087686, 197.85977686606589, 503.30633319757663, 579.5283660766967, 837.9551775675354, 436.5013682069731, 425.1640065860985, 522.498811954075, 787.5145000996667, 312.90159716303197, 240.33902220809955, 729.2575192358966, 1038.0932740504438, 330.19659478828333, 285.60808306511996, 153.6126644643762], 127: [677.8034563399877, 414.63152438896566, 505.5620030294063, 721.5488497821568, 625.5988825280571, 201.52462678216648, 872.9933636781392, 567.1874348254692, 513.1793283617236, 316.9482949400491, 228.73157046141262, 211.53197688206873, 227.65493672471757, 386.5781652473159, 375.0142202335159, 221.92021248173802, 514.7583461373138, 762.63463460227, 775.4245703306068, 290.89450426696715, 309.8401754458338], 167: [653.7936625094658, 647.0908069935614, 388.9161571326128, 757.5469325178472, 657.5489231022322, 440.6037374319461, 775.2152616616496, 1210.5048782399704, 309.74619336118405, 307.15465183311096, 102.89481566627546, 481.29683062971213, 615.5378621639363, 867.4912934000644, 338.02376959713047, 326.8319432728658, 467.7193826820153, 671.3485984642659, 345.0405396199166, 509.26999488616707, 586.8072993867152, 518.1556336129592, 194.12618232326727, 900.3987567184421, 834.9360555774145, 438.2639443877282, 683.6775912149867, 546.6704085031886, 261.2587028961935, 344.0218101355311, 613.3210263784691, 308.9304150943407, 549.7801516148212, 405.5140922766085, 449.16223402335856, 466.9280602777058, 496.7391510005817, 1269.4427630456041, 369.79888183408667], 523: [596.9186469151281, 246.4890503947042, 389.6917744146752, 30.598838284648746, 233.05953860822746, 721.3618573224205, 631.1423831366382, 375.8297831879157, 109.13710916309944, 569.1121440495224, 643.3140561058268, 243.54765886592168, 206.73743581900288, 344.5232130924697, 620.3572060416097, 515.8336379243157]}]\n",
    "popWeight = []\n",
    "for genome in genomeWeight:\n",
    "    print(f'\\n')\n",
    "    for depot in genome.keys():\n",
    "        #print(depot)\n",
    "        popWeight.append({depot:sum(genome[depot])})\n",
    "\n",
    "print(popWeight)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a7370fec6be111b608cc34d50d3192bee79c029552a2dcb4c2363d9b827a0176"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
